{"ast":null,"code":"var _jsxFileName = \"/Users/abhinavprasad/Desktop/ai-face-detection/src/App.js\",\n    _s = $RefreshSig$();\n\nimport { useRef, useEffect } from 'react';\nimport './App.css';\nimport * as faceapi from \"face-api.js\";\nimport { FaceExpressions } from 'face-api.js'; // new faceapi.TinyFaceDetectorOptions()\n\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nfunction App() {\n  _s();\n\n  const videoRef = useRef();\n  const canvasRef = useRef();\n  useEffect(() => {\n    startVideo();\n    videoRef && loadModels();\n  }, []);\n\n  const loadModels = () => {\n    Promise.all([faceapi.nets.tinyFaceDetector.loadFromUri('/models'), faceapi.nets.faceLandmark68Net.loadFromUri('/models'), faceapi.nets.faceRecognitionNet.loadFromUri('/models'), faceapi.nets.faceExpressionNet.loadFromUri('/models')]).then(() => {\n      faceDetection();\n    });\n  };\n\n  const startVideo = () => {\n    navigator.mediaDevices.getUserMedia({\n      video: true\n    }).then(currentStream => {\n      videoRef.current.srcObject = currentStream;\n    }).catch(err => {\n      console.error(err);\n    });\n  };\n\n  const faceDetection = async () => {\n    setInterval(async () => {\n      let number = 0;\n      const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\n      let emotions_displayed = detections[0].expressions;\n\n      if (emotions_displayed != undefined) {\n        number++;\n      }\n\n      let max_emotion = 0;\n      let final_emotion = \"\";\n\n      for (const [key, value] of Object.entries(emotions_displayed)) {\n        if (value > max_emotion) {\n          max_emotion = value;\n          final_emotion = key;\n        }\n      }\n\n      console.log(final_emotion + ':' + max_emotion);\n\n      if (number == 1) {\n        return;\n      }\n\n      canvasRef.current.innerHtml = faceapi.createCanvasFromMedia(videoRef.current);\n      faceapi.matchDimensions(canvasRef.current, {\n        width: 940,\n        height: 650\n      });\n      const resized = faceapi.resizeResults(detections, {\n        width: 940,\n        height: 650\n      });\n      faceapi.draw.drawDetections(canvasRef.current, resized);\n      faceapi.draw.drawFaceLandmarks(canvasRef.current, resized);\n      faceapi.draw.drawFaceExpressions(canvasRef.current, resized);\n    }, 1000);\n  };\n\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"app\",\n    children: [/*#__PURE__*/_jsxDEV(\"h1\", {\n      children: \" AI FACE DETECTION\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 83,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"h1\", {\n      children: \"  \"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 84,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"app__video\",\n      children: /*#__PURE__*/_jsxDEV(\"video\", {\n        crossOrigin: \"anonymous\",\n        ref: videoRef,\n        autoPlay: true\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 86,\n        columnNumber: 9\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 85,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"canvas\", {\n      ref: canvasRef,\n      width: \"940\",\n      height: \"650\",\n      className: \"app__canvas\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 88,\n      columnNumber: 9\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 82,\n    columnNumber: 5\n  }, this);\n}\n\n_s(App, \"0gwqVvoOV2or9Ql4L8GH2BGn3hc=\");\n\n_c = App;\nexport default App;\n\nvar _c;\n\n$RefreshReg$(_c, \"App\");","map":{"version":3,"sources":["/Users/abhinavprasad/Desktop/ai-face-detection/src/App.js"],"names":["useRef","useEffect","faceapi","FaceExpressions","App","videoRef","canvasRef","startVideo","loadModels","Promise","all","nets","tinyFaceDetector","loadFromUri","faceLandmark68Net","faceRecognitionNet","faceExpressionNet","then","faceDetection","navigator","mediaDevices","getUserMedia","video","currentStream","current","srcObject","catch","err","console","error","setInterval","number","detections","detectAllFaces","TinyFaceDetectorOptions","withFaceLandmarks","withFaceExpressions","emotions_displayed","expressions","undefined","max_emotion","final_emotion","key","value","Object","entries","log","innerHtml","createCanvasFromMedia","matchDimensions","width","height","resized","resizeResults","draw","drawDetections","drawFaceLandmarks","drawFaceExpressions"],"mappings":";;;AAAA,SAASA,MAAT,EAAiBC,SAAjB,QAAkC,OAAlC;AACA,OAAO,WAAP;AACA,OAAO,KAAKC,OAAZ,MAAyB,aAAzB;AACA,SAASC,eAAT,QAAgC,aAAhC,C,CACA;;;;AAEA,SAASC,GAAT,GAAe;AAAA;;AACb,QAAMC,QAAQ,GAAGL,MAAM,EAAvB;AACA,QAAMM,SAAS,GAAGN,MAAM,EAAxB;AAEAC,EAAAA,SAAS,CAAC,MAAM;AACdM,IAAAA,UAAU;AAEVF,IAAAA,QAAQ,IAAIG,UAAU,EAAtB;AAED,GALQ,EAKN,EALM,CAAT;;AAOE,QAAMA,UAAU,GAAG,MAAM;AACvBC,IAAAA,OAAO,CAACC,GAAR,CAAY,CACVR,OAAO,CAACS,IAAR,CAAaC,gBAAb,CAA8BC,WAA9B,CAA0C,SAA1C,CADU,EAEVX,OAAO,CAACS,IAAR,CAAaG,iBAAb,CAA+BD,WAA/B,CAA2C,SAA3C,CAFU,EAGVX,OAAO,CAACS,IAAR,CAAaI,kBAAb,CAAgCF,WAAhC,CAA4C,SAA5C,CAHU,EAIVX,OAAO,CAACS,IAAR,CAAaK,iBAAb,CAA+BH,WAA/B,CAA2C,SAA3C,CAJU,CAAZ,EAKGI,IALH,CAKQ,MAAM;AACZC,MAAAA,aAAa;AACd,KAPD;AAQD,GATD;;AAWF,QAAMX,UAAU,GAAG,MAAM;AACvBY,IAAAA,SAAS,CAACC,YAAV,CAAuBC,YAAvB,CAAoC;AAAEC,MAAAA,KAAK,EAAE;AAAT,KAApC,EACGL,IADH,CACSM,aAAD,IAAmB;AACvBlB,MAAAA,QAAQ,CAACmB,OAAT,CAAiBC,SAAjB,GAA6BF,aAA7B;AACD,KAHH,EAIGG,KAJH,CAIUC,GAAD,IAAS;AACdC,MAAAA,OAAO,CAACC,KAAR,CAAcF,GAAd;AACD,KANH;AAOD,GARD;;AAUA,QAAMT,aAAa,GAAG,YAAY;AAChCY,IAAAA,WAAW,CAAC,YAAW;AACrB,UAAIC,MAAM,GAAG,CAAb;AACA,YAAMC,UAAU,GAAG,MAAM9B,OAAO,CAAC+B,cAAR,CAAuB5B,QAAQ,CAACmB,OAAhC,EAAyC,IAAItB,OAAO,CAACgC,uBAAZ,EAAzC,EAAgFC,iBAAhF,GAAoGC,mBAApG,EAAzB;AACA,UAAIC,kBAAkB,GAAGL,UAAU,CAAC,CAAD,CAAV,CAAcM,WAAvC;;AACA,UAAID,kBAAkB,IAAIE,SAA1B,EACA;AACER,QAAAA,MAAM;AACP;;AACD,UAAIS,WAAW,GAAG,CAAlB;AACA,UAAIC,aAAa,GAAG,EAApB;;AACA,WAAK,MAAM,CAACC,GAAD,EAAKC,KAAL,CAAX,IAA0BC,MAAM,CAACC,OAAP,CAAeR,kBAAf,CAA1B,EACA;AACE,YAAIM,KAAK,GAAGH,WAAZ,EACA;AACEA,UAAAA,WAAW,GAAGG,KAAd;AACAF,UAAAA,aAAa,GAAGC,GAAhB;AACD;AACF;;AACDd,MAAAA,OAAO,CAACkB,GAAR,CAAYL,aAAa,GAAG,GAAhB,GAAsBD,WAAlC;;AACA,UAAIT,MAAM,IAAE,CAAZ,EACA;AACE;AACD;;AACDzB,MAAAA,SAAS,CAACkB,OAAV,CAAkBuB,SAAlB,GAA8B7C,OAAO,CAAC8C,qBAAR,CAA8B3C,QAAQ,CAACmB,OAAvC,CAA9B;AACAtB,MAAAA,OAAO,CAAC+C,eAAR,CAAwB3C,SAAS,CAACkB,OAAlC,EAA2C;AACzC0B,QAAAA,KAAK,EAAE,GADkC;AAEzCC,QAAAA,MAAM,EAAE;AAFiC,OAA3C;AAKA,YAAMC,OAAO,GAAGlD,OAAO,CAACmD,aAAR,CAAsBrB,UAAtB,EAAkC;AAChDkB,QAAAA,KAAK,EAAE,GADyC;AAEhDC,QAAAA,MAAM,EAAE;AAFwC,OAAlC,CAAhB;AAKAjD,MAAAA,OAAO,CAACoD,IAAR,CAAaC,cAAb,CAA4BjD,SAAS,CAACkB,OAAtC,EAA+C4B,OAA/C;AACAlD,MAAAA,OAAO,CAACoD,IAAR,CAAaE,iBAAb,CAA+BlD,SAAS,CAACkB,OAAzC,EAAkD4B,OAAlD;AACAlD,MAAAA,OAAO,CAACoD,IAAR,CAAaG,mBAAb,CAAiCnD,SAAS,CAACkB,OAA3C,EAAoD4B,OAApD;AAED,KAtCU,EAsCR,IAtCQ,CAAX;AAuCD,GAxCD;;AA0CA,sBACE;AAAM,IAAA,SAAS,EAAC,KAAhB;AAAA,4BACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YADF,eAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAFF,eAGE;AAAK,MAAA,SAAS,EAAC,YAAf;AAAA,6BACE;AAAO,QAAA,WAAW,EAAC,WAAnB;AAA+B,QAAA,GAAG,EAAE/C,QAApC;AAA8C,QAAA,QAAQ;AAAtD;AAAA;AAAA;AAAA;AAAA;AADF;AAAA;AAAA;AAAA;AAAA,YAHF,eAMI;AAAQ,MAAA,GAAG,EAAEC,SAAb;AAAwB,MAAA,KAAK,EAAC,KAA9B;AAAoC,MAAA,MAAM,EAAC,KAA3C;AAAiD,MAAA,SAAS,EAAC;AAA3D;AAAA;AAAA;AAAA;AAAA,YANJ;AAAA;AAAA;AAAA;AAAA;AAAA,UADF;AAWD;;GArFQF,G;;KAAAA,G;AAuFT,eAAeA,GAAf","sourcesContent":["import { useRef, useEffect } from 'react';\nimport './App.css';\nimport * as faceapi from \"face-api.js\";\nimport { FaceExpressions } from 'face-api.js';\n// new faceapi.TinyFaceDetectorOptions()\n\nfunction App() {\n  const videoRef = useRef();\n  const canvasRef = useRef();\n\n  useEffect(() => {\n    startVideo();\n\n    videoRef && loadModels();\n\n  }, []);\n  \n    const loadModels = () => {\n      Promise.all([\n        faceapi.nets.tinyFaceDetector.loadFromUri('/models'),\n        faceapi.nets.faceLandmark68Net.loadFromUri('/models'),\n        faceapi.nets.faceRecognitionNet.loadFromUri('/models'),\n        faceapi.nets.faceExpressionNet.loadFromUri('/models'),\n      ]).then(() => {\n        faceDetection();\n      })\n    };\n\n  const startVideo = () => {\n    navigator.mediaDevices.getUserMedia({ video: true })\n      .then((currentStream) => {\n        videoRef.current.srcObject = currentStream;\n      })\n      .catch((err) => {\n        console.error(err)\n      });\n  }\n\n  const faceDetection = async () => {\n    setInterval(async() => {\n      let number = 0;\n      const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\n      let emotions_displayed = detections[0].expressions;\n      if (emotions_displayed != undefined)\n      {\n        number++;\n      }\n      let max_emotion = 0\n      let final_emotion = \"\"\n      for (const [key,value] of Object.entries(emotions_displayed))\n      {\n        if (value > max_emotion)\n        {\n          max_emotion = value\n          final_emotion = key\n        }\n      }\n      console.log(final_emotion + ':' + max_emotion)\n      if (number==1)\n      {\n        return\n      }\n      canvasRef.current.innerHtml = faceapi.createCanvasFromMedia(videoRef.current);\n      faceapi.matchDimensions(canvasRef.current, {\n        width: 940,\n        height: 650,\n      })\n\n      const resized = faceapi.resizeResults(detections, {\n        width: 940,\n        height: 650,\n      });\n\n      faceapi.draw.drawDetections(canvasRef.current, resized)\n      faceapi.draw.drawFaceLandmarks(canvasRef.current, resized)\n      faceapi.draw.drawFaceExpressions(canvasRef.current, resized)\n\n    }, 1000)\n  }\n\n  return (\n    <div  className=\"app\">\n      <h1> AI FACE DETECTION</h1>\n      <h1>  </h1>\n      <div className='app__video'>\n        <video crossOrigin='anonymous' ref={videoRef} autoPlay ></video>\n      </div>\n        <canvas ref={canvasRef} width=\"940\" height=\"650\" className='app__canvas' />\n      \n    </div>\n  );\n}\n\nexport default App;\n"]},"metadata":{},"sourceType":"module"}